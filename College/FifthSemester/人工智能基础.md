## 无信息搜索

宽度优先：节点生成时进行目标检测，有完备性和最优性

一致代价：优先队列代替开节点表，节点从开节点表弹出时目标检测，不能存在负代价

深度优先：开节点表为堆栈，要做简单环图检查，不是最优、图不完备

深度受限：问题同上

迭代加深：完备、最优

双向：起点在搜索结果和终点搜索结果相交



## 有信息搜索

复杂度：当$n > n_{0}$时

大O：$f(n) \leq \text{cg}(n)$

小o：$f(n) \leq \text{cg}(n),\ \lim_{n\to \infty} \frac{f(n)}{g(n)} = 0$

$\Omega:f(n) \geq \text{cg}(n)$

$\omega:f(n) > cg(n),\ \lim_{n\to \infty} \frac{f(n)}{g(n)} = \infty$

$\Theta:c_{2}g(n) \leq f(n) \leq c_{1}g(n),\ \lim_{n\to \infty} \frac{f(n)}{g(n)} = \infty$

一个算法最坏情况是多项式复杂度问题容易

算法分类：优化问题、判定问题

非确定性算法(判定问题)：非确定性猜想+确定性验证

非确定性算法的验证阶段具有多项式时间复杂度为非确定性多项式算法(NP)

确定性算法多项式时间可解的判定问题：P类问题$P \subseteq \text{NP}$

A每一个实例都可转化为B，成为A可归约约为B，如果多项式时间内可完成，为多项式可归约。

NP完全问题：任何一个NP问题都能多项式归约到的一个特定NP问题

NP难题：任何一个NP问题都能多项式归约到的一个特定问题

无信息搜索复杂度：

宽搜：时间空间均为指数

一致代价：同上

深搜：时间指数，空间线性

迭代深搜：正确性类似广搜，复杂度类似深搜

启发函数：对当前状态最小代价的估计（非负）

贪婪最佳优先搜索：一致代价的代价函数换为启发函数，分支因子少，复杂度降低，不保证最优

A\*：代价函数为路径代价与启发函数的和

可采纳：$h(n) \leq h^{*}(n)$（真实代价）树搜索最优

证明：G最优H次优，则

$$
h(G) = h(H) = 0 \Rightarrow f(H) > f(G)
$$

$$
h(n) \leq h^{*}(n) \Rightarrow
$$

$$
f(G) = g(n) + h^{*}(n) \geq
$$

$$
g(n) + h(n) = f(n) \Rightarrow
$$

$$
f(H) > f(n)
$$

一致性：$h(n) \leq c\left( n,n^{'} \right) + h\left( n^{'} \right) \Rightarrow$图搜索最优

假设之前g最优，下一个扩展的节点不是最优，利用一致性证伪

证终点前一个点时可采纳的，数学归纳证明

一致性可采纳性（数学归纳）是单调的

空间复杂度仍然较高，改进：迭代加深A\*，递归最佳优先搜索（超过阈值就返回，用子节点代价更新阈值），内存受限A\*（内存耗尽后抛弃最差的节点）

当计算量允许应选用大的启发函数，启发函数设计：松弛问题：放宽条件

启发函数合成：$h(n) = max\{ h_{1}(n),\ldots,h_{k}(n)\}$，

方法：求解子问题、模式数据库（将子问题的解存于数据库）、从经验学习



## 约束满足

一组变量$X = \{X_1,\dots, X_n\}$描述一个状态

每个变量有自己的取值范围$D = \{D_1,\dots, D_n\},X_i\in D_i$和一定的约束$C=\{((X_i,\dots, X_j), restriction), \dots\}$

求解；约束传播，使用约束减少变量的取值范围

搜索，通过搜索算法对变量赋值

回溯搜索：每次选择一个变量赋值，若取值为空返回父节点尝试新取值。

赋值选择：最少剩余值、最多约束项

节点相容：值域取值满足一元约束

边相容：值域取值满足与该变量相关的二元相容，检测算法AC-3

如果$\{ X_{i},X_{j}\}$每一个相容赋值，$X_{m}$都有合适的取值使得其与两个变量都相容，则$\{ X_{i},X_{j}\}$对于$X_{m}$路径相容PC-2算法检测

k-相容：如果对任何k-1个变量的相容赋值，第k个变量总能被赋予一个和它们相容的值，则称这k-1个变量对于第k个变量是k相容的

全局约束：涉及多个变量的约束

搜索中的推理

每次决定某变量取值后，推理其邻居变量的值域

前向检查：变量赋值后检查边相容

智能回溯：回溯到产生冲突的变量

当基于赋值 X = x 的前向检验删除变量 Y 值域中的值时

应该把 X = x 加入到 Y 的冲突集

如果此时从Y 的值域中删除的是最后一个值，

应该把 Y 的冲突集中的每个赋值加入到 X 的冲突集

局部搜索：每个变量都赋一个值，一次改变一个变量的值

局部搜索方法：最小冲突数、爬山（像函数值增加方向移动）、模拟退火、遗传算法、梯度下降



## 对抗搜索

双人零和博弈

极小极大搜索

对方走需考虑所有方案，取效用值最小的，自己走取效用值最大的

指数时间复杂度、线性空间复杂度

截断搜索与剪枝

估值函数用于较为复杂的问题来近似求解效用值

终止状态：与真实效用一致

非终止：与取胜关联

Alpha剪枝：极大节点估值小于父节点值，剪掉该节点及其后继兄弟

Beta剪枝：相反

Max层更新$\alpha$值，使用$\beta$值剪裁下一层

Min层反之

分支因子从$b^{m} \rightarrow b^{m\text{/}2}$

蒙特卡洛搜索

仿真从节点开始的完整游戏（纯蒙特卡洛仿真/根据行动策略仿真）

选择叶子节点：随机选择、重要性选择。

扩展空叶子节点，根据仿真结果更新所有涉及节点

随机博弈：除极小极大还需增加随机层，节点数乘

随机层效用对所有可能值求期望，粗略估计可以进行剪枝



## 命题逻辑

描述事实是否成立的陈述句（特殊命题，原子命题，复合命题）$\neg,\  \land , \vee , \Rightarrow , \Leftrightarrow$

使$\alpha$为真的语句$\beta$也为真，则$\alpha \vDash \beta$，$\alpha$蕴含了$\beta$

模型检查算法：指数时间复杂度、线性空间复杂度，本质为回溯算法

演绎推理

等价演算：

$$
A \land (B \vee C) \equiv (A \land B) \vee (A \land C)
$$

$$
\neg(A \land B) \equiv \neg A \vee \neg B
$$

$$
A \Rightarrow B \equiv \neg A \vee B
$$

$$
\neg A \Rightarrow B \equiv \neg B \Rightarrow \neg A
$$

$$
A \Rightarrow B \equiv (A \Rightarrow B) \land (B \Rightarrow A)
$$

$$
(A \Rightarrow B) \land (A \Rightarrow \neg B) \equiv \neg A
$$

语句对所有模型都为真：有效性/重言式

$$
A \Rightarrow (A \vee B)
$$

$$
(A \land B) \Rightarrow A
$$

$$
\left( (A \Rightarrow B) \land \neg B \right) \Rightarrow \neg A
$$

$$
\left( (A \Rightarrow B) \land A \right) \Rightarrow B
$$

$$
\left( (A \vee B) \land \neg A \right) \Rightarrow B
$$

$$
\left( (A \Rightarrow B) \land (B \Rightarrow C) \right) \Rightarrow (A \Rightarrow C)
$$

$$
(A \Rightarrow B) \land (C \Rightarrow D) \land (A \vee C) \Rightarrow (B \vee D)
$$

限定子句，只含一个正文字的析取式，等价于隐含式

目标子句：不包含正文字的析取式

事实：仅包含一个正文字的限定子句

规则：包含若干反文字和一个正文字的限定子句

前向链接：限定子句上使用假言推理，从事实出发匹配规则，可能出现无关结果

与或图：半圆为与（合取）

后向链接：从查询除法倒推事实

归结原理

可满足性：某些模型下为真

不可满足：所有模型均为假

归谬法

$\alpha \vDash \beta$当且仅当$\alpha \land \neg\beta$不可满足

归结原理

$$
(\alpha \vee \beta) \land (\neg\alpha \vee \gamma) \vDash (\beta \vee \gamma)
$$

转化为合取范式：等价消去、隐含消去、否定深入、分配律

证明等价时可以将一侧等式转化为另一侧再证明

归结算法：欲证明$\text{KB} \vDash \alpha$，只需证$\text{KB} \land \lnot \alpha$不可满足，两个语句归结的结果为空子句则$\text{KB} \vDash \alpha$为真，若没有可以添加的新子句则为假

演绎推理对知识库有限定性要求、但可解释性强、直观、效率高，归结原理对知识库无限制，但不直观、效率稍低



## 谓词逻辑

命题逻辑：事实是否成立

谓词逻辑：关系是否成立

对象、关系、函数

符号

表示论域中对象、关系、函数的字母或单词

常量符号、谓词符号、函数符号

项：指代对象的表达式（常量符号和函数符号作用的结果

复合语句：逻辑连接符相连

全称量词，隐含合取可以很好地表示

存在量词，隐含析取可以很多好地表示

量词嵌套：

$$
\forall\text{xP}(x) \equiv \neg\exists x\neg P(x)
$$

$$
\forall x\neg P(x) \equiv \neg\exists\text{xP}(x)
$$

$$
\neg\forall\text{xP}(x) \equiv \exists x\neg P(x)
$$

等值演算：

$$
\forall x\left( P(x) \Rightarrow Q \right) \equiv \left( \exists\text{xP}(x) \right) \Rightarrow Q \equiv \exists\text{xP}(x) \Rightarrow Q
$$

其余情况保持不变

存在量词同理

全称量词只可对合取分配

存在量词只可对析取分配
$$
\begin{aligned}
&\forall x(P(x) \wedge Q(x)) \equiv \forall x P(x) \wedge \forall x Q(x) \\
&\exists x(P(x) \vee Q(x)) \equiv \exists x P(x) \vee \exists x Q(x)
\end{aligned}
$$
前束范式：所有量词都在最左侧且辖域囊括所有

全称量词可直接消去

存在量词置换为特定对象名，若被其他量词管辖，置换为相应函数。

转化为合取范式：等价消去、隐含消去、否定深入、变量换名、变量前移、量词消去（先存在后全称）、分配律

演绎推理：置换变量

合一：使多个语句变得相同



## 线性回归

$$
y\  = \ wx + b
$$

最小二乘法：

$s_{\text{xx}} = \sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}$

$$
\widehat{w} = \frac{s_{\text{xy}}}{s_{\text{xx}}},\widehat{b} = \overline{y} - \widehat{w}\overline{x}
$$

平方和分解

确定系数$r^{2} = \frac{S_{\text{xy}}^{2}}{S_{\text{xx}}S_{\text{yy}}}$

极大似然估计

假设检验

常用训练方法，多倍交叉验证

评价指标

$\text{MSE} = \frac{1}{n}\sum_{}^{}e^{2}$

$RMSE = \sqrt{\text{MSE}}$

$MAE = \frac{1}{n}\ \sum|e|$线性回归扩展：过原点的回归、带观测误差的回归（正交最小二乘）

多项式回归：最小二乘$w = \left( X^{T}X \right)^{- 1}X^{T}y$

简单模型容易欠拟合，复杂模型容易过拟合

正则化

降低模型复杂程度以提高泛化能力

Ridge回归：2范数，一定有解

Lasso：1范数，稀疏，大大简化

最优子集：0范数，非常稀疏，计算量过大

ElasticNet：1范数+2范数

步进回归：遍历所有，计算量大

前向选择：从简单模型开始，依次加入使模型性能提高最多的变量

后向选择：从最复杂的开始，删除一个......

双向选择



## Logistic回归

$$
P(Y = 1) = \pi(x) = \frac{1}{1 + e^{- (wx + b)}}
$$

$$
P(\pi = y) = \pi^{y}(1 - \pi)^{1 - y}
$$

似然$f\left( y \middle| w,b \right) = \prod_{i = 1}^{n}{\left( \pi\left( x_{i} \right) \right)^{y_{i}}\left( 1 - \pi\left( x_{i} \right) \right)^{1 - y_{i}}}$

信息熵$H = - \sum_{i = 1}^{l}{p_{i}\log_{2}p_{i}}$

交叉熵$- \frac{1}{n}\sum_{i = 1}^{n}\left( y_{i}\log\pi\left( x_{i} \right) + \left( 1 - y_{i} \right)\log\left( 1 - \pi\left( x_{i} \right) \right) \right)$

优化：最大化对数似然，最小化交叉熵

梯度下降：$x_{\text{new}} \leftarrow x - \alpha f^{'}(x)$

随机梯度下降：一次一个样例计算梯度

前向传播：从输入算输出

反向传播：输出算梯度（链式法则）

$\frac{\partial e}{\partial \pi} = \frac{\pi - y}{\pi(1 - \pi)},\ \frac{\partial\pi}{\partial z} = \pi(1 - \pi),\ \frac{\partial e}{\partial b} = \pi - y,\ \frac{\partial e}{\partial w} = (\pi - y)x$

TPR=TP/(TP+FN)

FPR=FP/(TN+FP)

TNR=TN/(TN+FP)

FNR=FN/(TP+FN)

Sensitivity=TP/(TP+FN)

Specificity=TN/(TN+FP)

Recall=TP/(TP+FN)

Precision=TP/(TP+FP)

Fall-out=FP/(TP+FP)

F1-measure=2×Precision×Recall/(Precision + Recall)

ACC=(TP+TN)/ALL

Balanced error rate：

$$
BER = \frac{1}{2}\left( \frac{\text{FP}}{FP + TN} + \frac{\text{FN}}{FN + TP} \right)
$$

PRC(Precision-Recall)

ROC(Sensitivity-(1-Specificity))

Softmax回归$\frac{e^{z_{j}}}{\sum_{k - 1}^{l}e^{z_{k}}}$

输出为独热码

交叉熵$- \frac{1}{n}\sum_{i = 1}^{n}{\sum_{j = 1}^{l}{y_{\text{ij}}\log\pi_{j}\left( x_{i} \right)}}$

$\frac{\partial e_k}{\partial \pi_k} = \frac{y_k}{\pi_k},\ \frac{\partial\pi_k}{\partial z_i} = \begin{cases}\pi_i(1 - \pi_i)\\-\pi_i\pi_k\end{cases},\ \frac{\partial e}{\partial b_{i}} = \pi_{i} - y_{i},\frac{\partial e}{\partial w_{i}} = \left( \pi_{i} - y_{i} \right)x$

$\text{PR}E_{\mu} = \frac{\sum_{k - 1}^{l}{TP_{k}}}{\sum_{k = 1}^{l}\left( TP_{k} + FP_{k} \right)}$

$\text{RE}C_{\mu} = \frac{\sum_{k - 1}^{l}TP_{k}}{\sum_{k = 1}^{l}\left( TP_{k} + FN_{k} \right)}$

$F_{\mu} = \frac{2PRE_{\mu} \times \text{RE}C_{\mu}}{\text{PR}E_{\mu} + REC_{\mu}}$

$\text{PR}E_{M} = \frac{1}{l}\sum_{k = 1}^{l}\frac{TP_{k}}{TP_{k} + FP_{k}}$

$\text{RE}C_{M} = \frac{1}{l}\sum_{k = 1}^{l}\frac{TP_{k}}{TP_{k} + FN_{k}}$

$F_{M} = \frac{2PRE_{M} \times \text{RE}C_{M}}{\text{PR}E_{M} + REC_{M}}$

$\overline{\text{ACC}} = \frac{1}{l}\sum_{k = 1}^{l}{\text{AC}C_{k}}$

$\overline{\text{ERR}} = \frac{1}{l}\sum_{k = 1}^{l}{\text{ER}R_{k}}$



## 前馈神经网络

输入单元：根据数据形成的向量

激活函数：Logistic、双曲正切$a=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$、ReLU

整流线性单元一般形式$a = \max(0,z) + \alpha\min(0,z)$

向量化前向传播

$$
z^{\lbrack i + 1\rbrack} = \left( b^{\lbrack z\rbrack}W^{\lbrack z\rbrack} \right)\begin{pmatrix}
1 \\
a^{\lbrack z\rbrack} \\
\end{pmatrix}
$$

反向传播：雅各比矩阵相乘

统计语言模型，假设一个词只与前面少数词相关

n-gram模型

$$
P(s) = P\left( w_{1} \right)P\left( w_{2}\mid w_{1} \right)P\left( w_{3}\mid w_{2} \right)\cdots P\left( w_{l}\mid w_{l - 1} \right)
$$

神经语言模型



**CNN**

特点：

稀疏连接，$O(N^2) \rightarrow O(m^2N)$

参数共享：卷积核扫描使用相同的参数$O(N^2)\to O(m^2)$

等变表示：局部特征位置不影响卷积结果

多卷积核$\to$多通道

特殊卷积核：非共享卷积（卷积核参数不同）、平铺卷积（非共享卷积多次重复）、1×1卷积核（通道合并与生成）

图卷积：将邻居节点加权求和

池化：最大池化、平均池化、随机池化

典型结构：LeNet、AlexNet、VGGNet(深网)、GoogleNet(多尺寸卷积)、ResNet(残差学习)

训练：

数据扩增：从原图抽取小块，对原图进行变换（几何、颜色、噪声）

动量：$\Delta \theta \propto v, \Delta v\propto -\nabla_\theta,v\leftarrow \alpha v-\epsilon \nabla_\theta,\theta \leftarrow \theta + v$

参数初始化策略：破坏对称性，随机初始化/借用已有结果

学习速率调整：线性衰减$\alpha_{k}= \begin{cases}\lambda \alpha_{\tau}+(1-\lambda) \alpha_{0} & k \leq \tau \\ \alpha_{\tau} & k>\tau\end{cases}$

自适应衰减：AdaGrad、RMSProp、Adam、学习速率反比于梯度累积平方和的平方根

正则化：增加数据、提前终止、修改目标函数（加入惩罚）、简化网络

批次标准化：$\hat x_i\leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma_B^2+\epsilon}},y_i\leftarrow \gamma \hat x_i + \beta$



## 强化学习

马尔可夫过程

状态转移概率$p_{ss'}=P(S_{t+1}=s'|S_t=s)$

分幕式任务：存在终止状态

持续性任务：没有终止状态

PageRank：$p=p^{(\infty)}=\lambda(I-(1-\lambda)W^T)^{-1}p^{(0)}$带重启的随机游走

马尔可夫回报过程

$\gamma$折现因子，$r_s$状态期望回报

$r_s=E[R_{t+1}|S_t=s]$

累积回报$G_t=\sum_{k=0}^\infty \gamma^kR_{t+k+1}$

状态价值函数：状态累积回报的期望

$v(s)=E[G_t|S_t=s]=r_s+\gamma\sum_{s'\in S} p_{ss'}v(s')$

贝尔曼期望方程$\mathbf{v=r+\gamma Pv\to v=(I-\gamma P)^{-1}r}$

马尔可夫决策过程

策略$\pi(a|s)=P(A_t=a|S_t=s)$

行动价值函数$q_\pi(s,a)=E[G_t|S_t=t, A_t=a]=r_s^a+\gamma\sum_{s'\in S} p_{ss'}^av_\pi(s'), v_\pi(s)=\sum_{a\in A} \pi(a|s)q_\pi(s, a)$

贝尔曼期望方程：$v_\pi(s)=\sum_{a\in A} \pi(a|s)(r_s^a+\gamma\sum_{s'\in S} p_{ss'}^av_\pi(s'))$

状态价值的贝尔曼期望方程：$q_{\pi}(s, a)=r_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathbf{S}} p_{s s^{\prime}}^{a} \sum_{a^{\prime} \in \mathbf{A}} \pi\left(a^{\prime} \mid s^{\prime}\right) q_{\pi}\left(s^{\prime}, a^{\prime}\right)$



## 策略迭代与价值迭代

策略评价：利用状态价值的贝尔曼期望方程进行迭代

$\mathbf{v^{(k+1)}=r^{\pi}+\gamma P^{\pi}v^{(k)}}$

对初值不敏感

策略迭代

最优状态价值和行动价值均在最优策略下取到

$\pi_*\ge \pi':v_{\pi_*}(s)\ge v_{\pi'}(s),\forall s,\forall \pi'$

最优状态价值即同时刻最优行动价值

$v_*(s)=\max_{a\in \mathbf{A}} q_*(s,a)$

贪心算法进行策略改进

$$
\begin{gather*}
\pi'(a|s)=
\begin{cases}
1 & \mathrm{if}\ a=\underset{a\in \mathbf{A}}{\arg\max}\ q_{\pi}(s, a)\\
0& \mathrm{otherwise}
\end{cases}\\
\pi'(s)=\underset{a\in \mathbf{A}}{\arg\max}\ q_{\pi}(s,a)\\
q_\pi(s,\pi'(s))=\max_{a \in \mathbf{A}} q_\pi(s,a)\\
v_\pi(s)=\sum_{a\in \mathbf{A}}\pi(a|s)q_\pi(s,a)\\
\le\sum_{a\in \mathbf{A}}\pi(a|s)q_\pi(s, \pi'(s))\\
=q_\pi(s,\pi'(s))\sum_{a\in \mathbf{A}}\pi(a|s)\\
v_\pi(s)\le q_\pi(s,\pi'(s))
\end{gather*}
$$

交替进行策略评价和策略改进

价值迭代：每次策略评价后立刻改进策略

$\mathbf{v^{(\mathit{k} + 1)}=\max_{\mathit{a}\in A}(r^\mathit{a}+\mathit{\gamma}P^\mathit{a}v^\mathit{(k)})}$

同步迭代和异步迭代

异步：$v(s)=\max _{a \in \mathbf{A}}\left(r_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathrm{S}} p_{s s^{\prime}}^{a} v\left(s^{\prime}\right)\right)$



## 蒙特卡洛与时序差分

用均值来近似数学期望和积分

根据待评价策略产生观测片段(到终止)计算$G_i$

首次访问蒙特卡洛、每次访问蒙特卡洛

$V(S_t)=\frac{1}{n}\sum_{i=1}^nG_i$

增量式蒙特卡洛预测$\bar X_k=\bar X_{k-1}+\frac{1}{k}(X_k-\bar X_{k-1})$

误差修正$V(S_t)\leftarrow V(S_t)+\alpha(G_t-V(S_t))$

蒙特卡洛控制

贪心策略优点状态行动访问不到，会陷入死循环

$\varepsilon$-贪心策略

$$
\pi(a|s)=
\begin{cases}
1-\varepsilon + \frac{\varepsilon}{m}& \mathrm{if}\ a=\underset{a\in \mathbf{A}}{\arg\max}\ Q(s,a)\\
\frac{\varepsilon}{m}& \mathrm{otherwise}
\end{cases}
$$

动态规划类似广搜，蒙特卡洛类似深搜

时序差分预测(从部分序列学习)

$V(S_t)\leftarrow V(S_t)+\alpha(R_{t+1}+\gamma V(S_{t+1})-V(S_t))$

将累积回报地推公式中的$G_{t+1}$近似为$V(S_{t+1})$

时序差分控制

$Q(S_t, A_t)\leftarrow Q(S_t, A_t)+\alpha(R_{t+1}+\gamma Q(S_{t+1}, A_{t+1})-Q(S_t, A_t))$

根据行为策略从$S_t$产生$A_t$，根据目标策略从$S_{t+1}$产生$A_{t+1}$计算$(S_{t+1},A_{t+1})$的行动价值更新$(S_t,A_t)$的行动价值

SARSAR(在线学习)：行为策略和目标策略均为$\varepsilon$-贪心

Q-Learning：行为策略为$\varepsilon$-贪心，目标策略为贪心

期望SARSAR：行为策略为$\varepsilon$-贪心，目标策略为行动的期望($\varepsilon$-贪心)



## 深度强化学习

状态太多表格法行不通

与普通监督学习比：输入不具独立性、不稳定、增量式输入

增量式价值近似

特征提取：基函数

多项式基函数：向量的每一元素为状态量一定幂次

$\hat v (s|\mathbf{x,w)=w^Tx}$

预测行动价值每个行动需要不同的参数

均方误差

$\min\sum_{i=1}^n(v_\pi(s_i)-\mathbf{w^Tx_\mathit{i})^2}$

不用对数似然因为假设过强

最小二乘法（离线学习）

$\mathbf{w=(X^\mathit{T}X)^{-1}X^\mathit{T}y}$

梯度下降$\mathbf{w^\mathit{(new)}\leftarrow w-\mathit{\alpha\nabla} w}$

$\nabla \mathbf{w}=\left(\frac{\partial L}{\partial w_{1}}, \cdots, \frac{\partial L}{\partial w_{m}}\right)^{T} \quad L=\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}-\sum_{j=1}^{m} w_{j} x_{j}^{[i]}\right)^{2}$

随机梯度下降$L=\left(y_{i}-\sum_{j=1}^{m} w_{j} x_{j}^{[i]}\right)^{2}$适合在线学习

$\mathbf{w}^{(\text {new})} \leftarrow \mathbf{w}+\alpha\left(v_{\pi}(s)-\hat{v}(s \mid \mathbf{x}, \mathbf{w})\right) \mathbf{x}$

$v_\pi(s)$用蒙特卡洛或者时序差分求出

行动价值同理

基于行动价值近似的控制

$\varepsilon$-贪心

简单易行、计算快速、适合在线学习

数据利用、经验利用不充分，预测目标变化快

批量式价值近似

离线学习：把当前数据存到一个库

从库里取一批数据来学习（经验回放）

在线学习方法、离线学习方法都可用

蒙特卡洛：

积累经验、交替进行经验采样+随机梯度下降

$D={(s_1,G_t(s_1),\dots, (s_n,G_t(s_n)))}$

也可直接用最小二乘直接求解$\mathbf{w=(X^\mathit{T}X)^\mathrm{-1}X^\mathit{T}g}$

时序差分：D中数据变为$(s_t,r_t',s_t')$

$\mathbf{w}=\left(\mathbf{X}^{T}\left(\mathbf{X}-\gamma \mathbf{X}^{\prime}\right)\right)^{-1} \mathbf{X}^{T} \mathbf{r}$

深度强化学习

DQN: Deep Q-Network

$\varepsilon$-贪心、经验回访、贪心、时序差分目标稳定

策略模型：为策略建立参数化模型

有更好的收敛性

